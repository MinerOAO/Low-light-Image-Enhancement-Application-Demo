INFO:  Checking Bread_onnx_optimized.onnx for usability with ORT Mobile.
INFO:  Checking NNAPI
INFO:  9 partitions with a total of 129/402 nodes can be handled by the NNAPI EP.
INFO:  Partition sizes: [16, 16, 16, 1, 16, 16, 16, 16, 16]
INFO:  Unsupported nodes due to operator=151
INFO:  Unsupported nodes due to input having a dynamic shape=122
INFO:  Unsupported ops: ai.onnx:Shape,ai.onnx:Split,com.microsoft.nchwc:Conv,com.microsoft.nchwc:GlobalAveragePool,com.microsoft.nchwc:MaxPool,com.microsoft.nchwc:ReorderInput,com.microsoft.nchwc:ReorderOutput,com.microsoft.nchwc:Upsample
DEBUG:  Caveats that have not been checked and may result in a node not being supported:  
     ai.onnx:Gather:Input indices should be constant if not int32 type.
     ai.onnx:Pad:Only constant mode Pad is supported. Input pads and constant_value should be constant. Input pads values should be non-negative.
     ai.onnx:Resize:Only 2D Resize is supported.
     ai.onnx:Squeeze:Input axes should be constant.
     ai.onnx:Unsqueeze:Input axes should be constant.
INFO:  NNAPI is not recommended with this model as there are 9 partitions covering 32.1% of the nodes in the model. This will most likely result in worse performance than just using the CPU EP.
INFO:  Model should perform well with NNAPI as is: NO
INFO:  Checking if model will perform better if the dynamic shapes are fixed...
INFO:  Partition information if the model was updated to make the shapes fixed:
INFO:  24 partitions with a total of 251/402 nodes can be handled by the NNAPI EP.
INFO:  Partition sizes: [18, 1, 26, 20, 20, 14, 26, 20, 20, 5, 1, 1, 3, 1, 3, 2, 20, 1, 2, 20, 1, 1, 7, 18]
INFO:  Unsupported nodes due to operator=151
INFO:  Unsupported ops: ai.onnx:Shape,ai.onnx:Split,com.microsoft.nchwc:Conv,com.microsoft.nchwc:GlobalAveragePool,com.microsoft.nchwc:MaxPool,com.microsoft.nchwc:ReorderInput,com.microsoft.nchwc:ReorderOutput,com.microsoft.nchwc:Upsample
DEBUG:  Caveats that have not been checked and may result in a node not being supported:  
     ai.onnx:Gather:Input indices should be constant if not int32 type.
     ai.onnx:Pad:Only constant mode Pad is supported. Input pads and constant_value should be constant. Input pads values should be non-negative.
     ai.onnx:Resize:Only 2D Resize is supported.
     ai.onnx:Squeeze:Input axes should be constant.
     ai.onnx:Unsqueeze:Input axes should be constant.
INFO:  NNAPI is not recommended with this model as there are 24 partitions covering 62.4% of the nodes in the model. This will most likely result in worse performance than just using the CPU EP.
INFO:  Model should perform well with NNAPI if modified to have fixed input shapes: NO
INFO:  Checking CoreML
INFO:  17 partitions with a total of 41/402 nodes can be handled by the CoreML EP.
INFO:  Partition sizes: [3, 2, 3, 2, 3, 2, 1, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2]
INFO:  Unsupported nodes due to operator=324
INFO:  Unsupported nodes due to input having a dynamic shape=37
INFO:  Unsupported ops: ai.onnx:Div,ai.onnx:Exp,ai.onnx:Gather,ai.onnx:Mul,ai.onnx:Neg,ai.onnx:Pad,ai.onnx:Pow,ai.onnx:Shape,ai.onnx:Slice,ai.onnx:Split,ai.onnx:Sub,ai.onnx:Unsqueeze,com.microsoft.nchwc:Conv,com.microsoft.nchwc:GlobalAveragePool,com.microsoft.nchwc:MaxPool,com.microsoft.nchwc:ReorderInput,com.microsoft.nchwc:ReorderOutput,com.microsoft.nchwc:Upsample
INFO:  CoreML is not recommended with this model as there are 17 partitions covering 10.2% of the nodes in the model. This will most likely result in worse performance than just using the CPU EP.
INFO:  Model should perform well with CoreML as is: NO
INFO:  Checking if model will perform better if the dynamic shapes are fixed...
INFO:  Partition information if the model was updated to make the shapes fixed:
INFO:  40 partitions with a total of 78/402 nodes can be handled by the CoreML EP.
INFO:  Partition sizes: [2, 2, 1, 1, 3, 2, 1, 3, 2, 1, 3, 2, 1, 1, 2, 3, 3, 2, 1, 3, 2, 1, 3, 2, 1, 1, 3, 1, 3, 2, 1, 1, 3, 2, 1, 2, 3, 3, 2, 2]
INFO:  Unsupported nodes due to operator=324
INFO:  Unsupported ops: ai.onnx:Div,ai.onnx:Exp,ai.onnx:Gather,ai.onnx:Mul,ai.onnx:Neg,ai.onnx:Pad,ai.onnx:Pow,ai.onnx:Shape,ai.onnx:Slice,ai.onnx:Split,ai.onnx:Sub,ai.onnx:Unsqueeze,com.microsoft.nchwc:Conv,com.microsoft.nchwc:GlobalAveragePool,com.microsoft.nchwc:MaxPool,com.microsoft.nchwc:ReorderInput,com.microsoft.nchwc:ReorderOutput,com.microsoft.nchwc:Upsample
INFO:  CoreML is not recommended with this model as there are 40 partitions covering 19.4% of the nodes in the model. This will most likely result in worse performance than just using the CPU EP.
INFO:  Model should perform well with CoreML if modified to have fixed input shapes: NO
INFO:  ---------------
INFO:  Checking if pre-built ORT Mobile package can be used with Bread_onnx_optimized.onnx once model is converted from ONNX to ORT format using onnxruntime.tools.convert_onnx_models_to_ort...
DEBUG:  Checking if the data types and operators used in the model are supported in the pre-built ORT package...
INFO:  Unsupported operators:
INFO:    com.microsoft.nchwc:1:Conv
INFO:    com.microsoft.nchwc:1:GlobalAveragePool
INFO:    com.microsoft.nchwc:1:MaxPool
INFO:    com.microsoft.nchwc:1:ReorderInput
INFO:    com.microsoft.nchwc:1:ReorderOutput
INFO:    com.microsoft.nchwc:1:Upsample
INFO:  
Model is not supported by the pre-built package due to unsupported types and/or operators.
INFO:  Please see https://onnxruntime.ai/docs/reference/mobile/prebuilt-package/ for information on what is supported in the pre-built package.
INFO:  A custom build of ONNX Runtime will be required to run the model. Please see https://onnxruntime.ai/docs/build/custom.html for details on performing that.
INFO:  ---------------

INFO:  Run `python -m onnxruntime.tools.convert_onnx_models_to_ort ...` to convert the ONNX model to ORT format. By default, the conversion tool will create an ORT format model with saved optimizations which can potentially be applied at runtime (with a .with_runtime_opt.ort file extension) for use with NNAPI or CoreML, and a fully optimized ORT format model (with a .ort file extension) for use with the CPU EP.
INFO:  For optimal performance the <model>.ort model should be used with the CPU EP. 